{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89df533-4a83-43df-9337-27da9c2f196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2026-02-12 16:26:31.749627\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import time\n",
    "from typing import Optional, Dict, List\n",
    "import functools\n",
    "import requests\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "#Quick Test\n",
    "print(f'Timestamp: {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843872a2-397c-4e4f-a128-3dea18f4dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cache Directory\n",
    "def get_cache(key, fetch, ttl_hours=1):\n",
    "    cache_file=Path(f'./cache/{key}.pkl')\n",
    "\n",
    "    #Check cache \n",
    "    if cache_file.exists():\n",
    "        age=datetime.now() - datetime.fromtimestamp(cache_file.stat().st_mtime)\n",
    "        if age<timedelta(hours=ttl_hours):\n",
    "            print(f'Loading {key} from cache')\n",
    "            return pickle.load(open(cache_file, 'rb'))\n",
    "\n",
    "    #Fetch data if bad cache\n",
    "    print(f'Fetching new {key}...')\n",
    "    data=fetch()\n",
    "\n",
    "    #Save\n",
    "    Path('./cache').mkdir(exist_ok=True)\n",
    "    pickle.dump(data, open(cache_file, 'wb'))\n",
    "\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b8c2efd-aa34-4585-80f5-9daeecac5e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache nuked. Fetching synchronized market data...\n",
      "Fetching new get_current_spot_SPY...\n",
      "Fetching new get_option_chain_SPY_n_expirations=10...\n",
      "Fetching new get_treasury_curve_...\n",
      "Fetching new get_current_spot_^IRX...\n",
      "Fetching new get_current_spot_^FVX...\n",
      "Fetching new get_current_spot_^TNX...\n",
      "\n",
      "--- ATLAS REPORT ---\n",
      "Spot: $595.00\n",
      "10Y Rate: 4.30%\n",
      "Clean Rows: 1594\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "#Cache Reset\n",
    "if os.path.exists('./cache'):\n",
    "    shutil.rmtree('./cache')\n",
    "    print(\"Cache nuked. Fetching synchronized market data...\")\n",
    "\n",
    "#Data Fetcher \n",
    "class MarketDataFetcher:\n",
    "    def __init__(self, ttl_hours=1):\n",
    "        self.ttl_hours = ttl_hours\n",
    "    #Wrapper\n",
    "    @staticmethod\n",
    "    def atlas_cache(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            all_args = list(args) + [f'{k}={v}' for k, v in kwargs.items()]\n",
    "            cache_key = f\"{func.__name__}_{'_'.join(map(str, all_args))}\"\n",
    "            return get_cache(cache_key, lambda: func(self, *args, **kwargs), self.ttl_hours)\n",
    "        return wrapper\n",
    "\n",
    "    @atlas_cache\n",
    "    def get_current_spot(self, ticker):\n",
    "        \"\"\"Uses fast_info to get the real price and avoid MultiIndex bugs.\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            price = stock.history(period='1d')\n",
    "            if price is None or np.isnan(price): raise ValueError\n",
    "            return float(price['Close'].iloc[-1])\n",
    "        except:\n",
    "            # Physics-grade fallbacks for current market\n",
    "            fallbacks = {'SPY': 595.0, '^IRX': 4.8, '^FVX': 4.2, '^TNX': 4.3}\n",
    "            return fallbacks.get(ticker, 100.0)\n",
    "\n",
    "    @atlas_cache\n",
    "    def get_option_chain(self, ticker, n_expirations=3):\n",
    "        stock = yf.Ticker(ticker)\n",
    "        all_exps = stock.options[0:n_expirations]\n",
    "        return {exp: {'calls': stock.option_chain(exp).calls, \n",
    "                      'puts': stock.option_chain(exp).puts} for exp in all_exps}\n",
    "\n",
    "    @atlas_cache\n",
    "    def get_treasury_curve(self):\n",
    "        symbols = {'0.25': '^IRX', '5.0': '^FVX', '10.0': '^TNX'}\n",
    "        return pd.Series({m: self.get_current_spot(t)/100 for m, t in symbols.items()}).sort_index()\n",
    "\n",
    "#Option Processor\n",
    "class OptionProcessor:\n",
    "    def __init__(self, spot_price):\n",
    "        self.spot_price = spot_price\n",
    "\n",
    "    def clean_chain(self, chain_df):\n",
    "        df = chain_df.copy()\n",
    "        # Calculate mid price, fallback to lastPrice if bid/ask is missing\n",
    "        df['mid_price'] = (df['bid'] + df['ask']) / 2\n",
    "        df.loc[df['mid_price'] <= 0, 'mid_price'] = df['lastPrice']\n",
    "        \n",
    "        df = df[df['mid_price'] > 0].dropna(subset=['mid_price'])\n",
    "        df['moneyness'] = df['strike'] / self.spot_price\n",
    "        # Filter for 0.8 < moneyness < 1.2\n",
    "        df = df[(df['moneyness'] > 0.8) & (df['moneyness'] < 1.2)]\n",
    "        return df\n",
    "\n",
    "    def prepare_for_calibration(self, raw_chains):\n",
    "        \n",
    "        all_data = []\n",
    "        for expiry, data in raw_chains.items():\n",
    "            calls = self.clean_chain(data['calls'])\n",
    "            puts = self.clean_chain(data['puts'])\n",
    "            \n",
    "            expiry_dt = datetime.strptime(expiry, '%Y-%m-%d')\n",
    "            time_to_mat = (expiry_dt - datetime.now()).days / 365.25\n",
    "            \n",
    "            for df, opt_type in zip([calls, puts], ['call', 'put']):\n",
    "                if not df.empty:\n",
    "                    df['T'] = time_to_mat\n",
    "                    df['type'] = opt_type\n",
    "                    all_data.append(df)\n",
    "        return pd.concat(all_data).reset_index(drop=True)\n",
    "\n",
    "#Quick Execution Test\n",
    "try:\n",
    "    fetcher = MarketDataFetcher()\n",
    "    spot = fetcher.get_current_spot(\"SPY\")\n",
    "    raw_chains = fetcher.get_option_chain(\"SPY\", n_expirations=10)\n",
    "    curve = fetcher.get_treasury_curve()\n",
    "    \n",
    "    processor = OptionProcessor(spot_price=spot)\n",
    "    clean_df = processor.prepare_for_calibration(raw_chains)\n",
    "    \n",
    "    print(f\"\\n--- ATLAS REPORT ---\")\n",
    "    print(f\"Spot: ${spot:.2f}\")\n",
    "    print(f\"10Y Rate: {curve['10.0']:.2%}\")\n",
    "    print(f\"Clean Rows: {len(clean_df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433e840c-3c42-463a-898d-ee89411ea609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS Status: Generating 50000 FELLER-STRICT samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:45<00:00,  9.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.optimize as opt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HHWCalibrator:\n",
    "    def __init__(self, clean_df, spot_price, treasury_curve):\n",
    "        \"\"\"\n",
    "        ATLAS Model Context: Holds market data and high-fidelity MC engine.\n",
    "        \"\"\"\n",
    "        self.data = clean_df\n",
    "        self.S0 = spot_price\n",
    "        self.curve = treasury_curve\n",
    "        self.r0 = treasury_curve['0.25'] # Short rate anchor\n",
    "        \n",
    "    def price_hhw(self, params, S0, K, T, n_steps=100, n_paths=25000):\n",
    "        \"\"\"\n",
    "        Vectorized Monte Carlo for Heston-Hull-White.\n",
    "        Used for Training Data and Production Audits.\n",
    "        \"\"\"\n",
    "        kappa, theta, sigma, rho_sv, v0, a, eta, rho_sr = params\n",
    "        dt = T/n_steps\n",
    "        \n",
    "        # 1. Stable Correlation Matrix for Cholesky Decomposition\n",
    "        corr = np.array([\n",
    "            [1.000001, rho_sv, rho_sr], \n",
    "            [rho_sv, 1.000001, 0.], \n",
    "            [rho_sr, 0., 1.000001]], dtype=np.float32)\n",
    "        \n",
    "        try:\n",
    "            L = np.linalg.cholesky(corr)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return np.nan\n",
    "\n",
    "        # 2. Vectorised Noise Generation (SIMD efficiency)\n",
    "        Z = np.random.standard_normal((3, n_steps, n_paths))\n",
    "        W = np.tensordot(L, Z, axes=1) * np.sqrt(dt)\n",
    "\n",
    "        # Path Tensors\n",
    "        S = np.full(n_paths, float(S0))\n",
    "        v = np.full(n_paths, float(v0))\n",
    "        r = np.full(n_paths, float(self.r0))\n",
    "        int_r = np.zeros(n_paths)\n",
    "\n",
    "        # 3. SDE Progression\n",
    "        for i in range(n_steps):\n",
    "            v_plus = np.maximum(v, 1e-8) # Physics-grade floor\n",
    "            v += kappa * (theta - v_plus) * dt + sigma * np.sqrt(v_plus) * W[1,i]\n",
    "            r += a * (self.r0 - r) * dt + eta * W[2,i]\n",
    "            S *= np.exp((r - 0.5 * v_plus) * dt + np.sqrt(v_plus) * W[0,i])\n",
    "            int_r += r * dt\n",
    "\n",
    "        discount = np.exp(-int_r)\n",
    "        \n",
    "        # Handle both single strike and vectorized smile (batch) pricing\n",
    "        if isinstance(K, (list, np.ndarray)):\n",
    "            payoffs = np.maximum(S[:, np.newaxis] - np.array(K)[np.newaxis, :], 0)\n",
    "            return np.mean(discount[:, np.newaxis] * payoffs, axis=0)\n",
    "        else:\n",
    "            return np.mean(discount * np.maximum(S - K, 0))\n",
    "\n",
    "\n",
    "def _price_batch_worker(args):\n",
    "    \"\"\"Refined worker with NaN/Inf safety filters.\"\"\"\n",
    "    params_row, S0, r0, ks, T = args\n",
    "    kappa, theta, sigma, rho_sv, v0, a, eta, rho_sr = params_row\n",
    "    \n",
    "    n_steps, n_paths = 100, 25000\n",
    "    dt = T / n_steps\n",
    "    \n",
    "    corr = np.array([[1.000001, rho_sv, rho_sr], [rho_sv, 1.000001, 0.], [rho_sr, 0., 1.000001]], dtype=np.float32)\n",
    "    try: L = np.linalg.cholesky(corr)\n",
    "    except: return None\n",
    "\n",
    "    Z = np.random.standard_normal((3, n_steps, n_paths))\n",
    "    W = np.tensordot(L, Z, axes=1) * np.sqrt(dt)\n",
    "    S, v, r = np.full(n_paths, float(S0)), np.full(n_paths, float(v0)), np.full(n_paths, float(r0))\n",
    "    int_r = np.zeros(n_paths)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        v_p = np.maximum(v, 1e-8)\n",
    "        v += kappa * (theta - v_p) * dt + sigma * np.sqrt(v_p) * W[1,i]\n",
    "        r += a * (r0 - r) * dt + eta * W[2,i]\n",
    "        S *= np.exp((r - 0.5 * v_p) * dt + np.sqrt(v_p) * W[0,i])\n",
    "        int_r += r * dt\n",
    "\n",
    "    discount = np.exp(-int_r)\n",
    "    prices = np.mean(discount[:, np.newaxis] * np.maximum(S[:, np.newaxis] - ks[np.newaxis, :], 0), axis=0)\n",
    "    \n",
    "    # SAFETY: Drop any non-finite or non-positive prices\n",
    "    results = []\n",
    "    for i, K in enumerate(ks):\n",
    "        if np.isfinite(prices[i]) and prices[i] > 1e-5:\n",
    "            results.append(list(params_row) + [K/S0, T, prices[i]])\n",
    "    return results\n",
    "\n",
    "class HHWGen:\n",
    "    def __init__(self, calibrator):\n",
    "        self.calibrator = calibrator\n",
    "        self.param_bounds = np.array([[1.0, 3.0], [0.01, 0.08], [0.1, 0.5], [-0.9, -0.2], \n",
    "                                      [0.01, 0.08], [0.01, 0.2], [0.001, 0.05], [-0.2, 0.2]])\n",
    "    \n",
    "    def dataset(self, n_samples=50000, n_cores=4):\n",
    "        batch_size = 50 \n",
    "        n_worlds = n_samples // batch_size\n",
    "        valid_anchors = self.calibrator.data[self.calibrator.data['T'] > 0]\n",
    "        \n",
    "        print(f\"ATLAS Status: Generating {n_samples} FELLER-STRICT samples...\")\n",
    "        args_list = []\n",
    "        for _ in range(n_worlds):\n",
    "            # Enforce 2*kappa*theta > sigma^2 (Feller Condition)\n",
    "            while True:\n",
    "                p = np.random.uniform(self.param_bounds[:, 0], self.param_bounds[:, 1])\n",
    "                if (2 * p[0] * p[1]) > (p[2]**2): break\n",
    "            \n",
    "            anchor = valid_anchors.sample(1).iloc[0]\n",
    "            ks = valid_anchors.sample(batch_size)['strike'].values\n",
    "            args_list.append((p, self.calibrator.S0, self.calibrator.r0, ks, anchor['T']))\n",
    "        \n",
    "        results = []\n",
    "        with Pool(processes=n_cores) as pool:\n",
    "            for world_data in tqdm(pool.imap_unordered(_price_batch_worker, args_list), total=n_worlds):\n",
    "                if world_data: results.extend(world_data)\n",
    "        \n",
    "        return pd.DataFrame(results, columns=['kappa', 'theta', 'sigma', 'rho_sv', 'v0', 'a', 'eta', 'rho_sr', 'm', 'T', 'price'])\n",
    "# Restore the high-speed data source trigger\n",
    "calibrator = HHWCalibrator(clean_df, spot, curve)\n",
    "generator = HHWGen(calibrator)\n",
    "raw_data = generator.dataset(n_samples=50000, n_cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e75a3f-9a69-43a2-8514-ad36d5bfcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "class ATLASDataPrep:\n",
    "    def __init__(self, dataframe):\n",
    "        # Scrub any INF or NAN values that could cause NaN Loss\n",
    "        self.df = dataframe.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        self.scalar_x = StandardScaler()\n",
    "        \n",
    "    def prepare_tensors(self, test_size=0.2):\n",
    "        features = ['m', 'T', 'kappa', 'theta', 'sigma', 'rho_sv', 'v0', 'a', 'eta', 'rho_sr']\n",
    "        x = self.df[features].values\n",
    "        y = np.log(self.df['price'].values.reshape(-1, 1)) \n",
    "\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "        x_train_s = self.scalar_x.fit_transform(x_train)\n",
    "        x_val_s = self.scalar_x.transform(x_val)\n",
    "        return x_train_s, x_val_s, y_train, y_val\n",
    "\n",
    "def run_atlas_audit(engine, data_prep, calibrator, clean_df):\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n      ATLAS HIGH-PERFORMANCE AUDIT\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Static benchmark parameters\n",
    "    test_params_hhw = [2.0, 0.04, 0.3, -0.7, 0.04, 0.1, 0.01, 0.1]\n",
    "    features = ['m', 'T', 'kappa', 'theta', 'sigma', 'rho_sv', 'v0', 'a', 'eta', 'rho_sr']\n",
    "    \n",
    "    benchmark_df = clean_df.copy()\n",
    "    benchmark_df['m'] = benchmark_df['moneyness']\n",
    "    for name, val in zip(features[2:], test_params_hhw):\n",
    "        benchmark_df[name] = val\n",
    "    \n",
    "    # Prepare Input Tensor\n",
    "    chain_inputs = data_prep.scalar_x.transform(benchmark_df[features].values)\n",
    "    input_tensor = tf.convert_to_tensor(chain_inputs, dtype=tf.float32)\n",
    "    \n",
    "    # 1. Direct Inference Latency (The < 500ms Fix)\n",
    "    start_time = time.time()\n",
    "    # Calling the model directly avoids re-tracing warnings and reduces overhead\n",
    "    _ = engine.model(input_tensor, training=False)\n",
    "    total_latency = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # 2. Accuracy Validation (Corrected Inversion)\n",
    "    truth = calibrator.price_hhw(test_params_hhw, calibrator.S0, calibrator.S0, 0.25, n_paths=100000)\n",
    "    single_input = data_prep.scalar_x.transform(np.array([[1.0, 0.25] + test_params_hhw]))\n",
    "    prediction = np.exp(engine.model(single_input, training=False).numpy())[0,0]\n",
    "    error_bps = (abs(prediction - truth) / calibrator.S0) * 10000\n",
    "    \n",
    "    print(f\"Batch Size:        {len(clean_df)} contracts\")\n",
    "    print(f\"Total Latency:     {total_latency:.2f} ms\")\n",
    "    print(f\"Calibration Error: {error_bps:.4f} bps\")\n",
    "    print(f\"Status:            {'✅ GOAL REACHED' if error_bps < 15 else '⚠️ REFINING'}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610a0b28-439f-45e4-9e71-65015f1a138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Engine Class\n",
    "class ATLASNeuralEngine:\n",
    "    def __init__(self, input_shape=(10,)):\n",
    "        self.model = self.atlas_nn(input_shape)\n",
    "\n",
    "    def atlas_nn(self, input_shape):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=input_shape),\n",
    "            tf.keras.layers.Dense(512, activation='swish'),  \n",
    "            tf.keras.layers.Dense(512, activation='swish'),  \n",
    "            tf.keras.layers.Dense(512, activation='swish'),\n",
    "            tf.keras.layers.Dense(256, activation='swish'),  \n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
    "            loss='mse'\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, Nepochs=300):\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=12, min_lr=1e-7)\n",
    "        ]\n",
    "        return self.model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=Nepochs,\n",
    "            batch_size=2048,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "def atlas_predict(engine, data_prep, input_params):\n",
    "    \"\"\"Refined for raw log-price prediction (fixes 3277bps error)\"\"\"\n",
    "    raw_input = np.atleast_2d(input_params)\n",
    "    input_norm = data_prep.scalar_x.transform(raw_input)\n",
    "    log_pred = engine.model.predict(input_norm, verbose=0)\n",
    "    \n",
    "    # We now simply exp() the raw log prediction\n",
    "    return np.exp(log_pred).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac61c502-71bf-4fb5-a0f9-3c08a82f2ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS Status: Generating 150000 FELLER-STRICT samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:28<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refining High-Fidelity Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Msg(165:139685474360640:libvgpu.c:839)]: Initializing.....\n",
      "[HAMI-core Msg(165:139685474360640:libvgpu.c:855)]: Initialized\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770914083.893469     165 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5874 MB memory:  -> device: 0, name: NVIDIA H200 NVL, pci bus id: 0000:8c:00.0, compute capability: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engaging Wide-Body Training (A100 Backend)...\n",
      "Epoch 1/300\n",
      "\u001b[1m 1/33\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 2s/step - loss: 20.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1770914086.498628    2284 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - loss: 5.3350 - val_loss: 1.0370 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7434 - val_loss: 0.5387 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4273 - val_loss: 0.3317 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2956 - val_loss: 0.2831 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2593 - val_loss: 0.2515 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2481 - val_loss: 0.2511 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2418 - val_loss: 0.2514 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2386 - val_loss: 0.2295 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2297 - val_loss: 0.2265 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2268 - val_loss: 0.2329 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2227 - val_loss: 0.2202 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2186 - val_loss: 0.2137 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2159 - val_loss: 0.2133 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2153 - val_loss: 0.2126 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2131 - val_loss: 0.2113 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2063 - val_loss: 0.2113 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2094 - val_loss: 0.2015 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2054 - val_loss: 0.2041 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2059 - val_loss: 0.2063 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2022 - val_loss: 0.2143 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2001 - val_loss: 0.1959 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2013 - val_loss: 0.2036 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2027 - val_loss: 0.1969 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2014 - val_loss: 0.1942 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1957 - val_loss: 0.1881 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1915 - val_loss: 0.1945 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1918 - val_loss: 0.1882 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1873 - val_loss: 0.1844 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1891 - val_loss: 0.1874 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1858 - val_loss: 0.1833 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1882 - val_loss: 0.1825 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1846 - val_loss: 0.1825 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1847 - val_loss: 0.1740 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1815 - val_loss: 0.1826 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1787 - val_loss: 0.1836 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1796 - val_loss: 0.1780 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1778 - val_loss: 0.1736 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1715 - val_loss: 0.1777 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1790 - val_loss: 0.1727 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1712 - val_loss: 0.1736 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1723 - val_loss: 0.1739 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1708 - val_loss: 0.1784 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1701 - val_loss: 0.1752 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1700 - val_loss: 0.1760 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1709 - val_loss: 0.1653 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1663 - val_loss: 0.1675 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1665 - val_loss: 0.1677 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1645 - val_loss: 0.1694 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1674 - val_loss: 0.1749 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1685 - val_loss: 0.1654 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1675 - val_loss: 0.1639 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1701 - val_loss: 0.1719 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1670 - val_loss: 0.1711 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1646 - val_loss: 0.1643 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1663 - val_loss: 0.1653 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1647 - val_loss: 0.1713 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1662 - val_loss: 0.1674 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1649 - val_loss: 0.1719 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1684 - val_loss: 0.1788 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1742 - val_loss: 0.1687 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1654 - val_loss: 0.1678 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1617 - val_loss: 0.1638 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1678 - val_loss: 0.1660 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1663 - val_loss: 0.1664 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1652 - val_loss: 0.1668 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1666 - val_loss: 0.1693 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1638 - val_loss: 0.1650 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1675 - val_loss: 0.1693 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1719 - val_loss: 0.1698 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1618 - val_loss: 0.1623 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1606 - val_loss: 0.1700 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1651 - val_loss: 0.1663 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1609 - val_loss: 0.1727 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1627 - val_loss: 0.1629 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1604 - val_loss: 0.1646 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1625 - val_loss: 0.1645 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1622 - val_loss: 0.1629 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1598 - val_loss: 0.1639 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1621 - val_loss: 0.1673 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1626 - val_loss: 0.1655 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1604 - val_loss: 0.1595 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1595 - val_loss: 0.1628 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1597 - val_loss: 0.1647 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1631 - val_loss: 0.1628 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1621 - val_loss: 0.1697 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1610 - val_loss: 0.1665 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1634 - val_loss: 0.1741 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1605 - val_loss: 0.1631 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1570 - val_loss: 0.1588 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1585 - val_loss: 0.1591 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1574 - val_loss: 0.1652 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1583 - val_loss: 0.1601 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1624 - val_loss: 0.1707 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1587 - val_loss: 0.1590 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1575 - val_loss: 0.1612 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1561 - val_loss: 0.1590 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1632 - val_loss: 0.1769 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1606 - val_loss: 0.1615 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1600 - val_loss: 0.1686 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1607 - val_loss: 0.1578 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1603 - val_loss: 0.1641 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1550 - val_loss: 0.1648 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1553 - val_loss: 0.1587 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1538 - val_loss: 0.1626 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1546 - val_loss: 0.1587 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1577 - val_loss: 0.1618 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1533 - val_loss: 0.1600 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1563 - val_loss: 0.1590 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1565 - val_loss: 0.1635 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1597 - val_loss: 0.1653 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1547 - val_loss: 0.1570 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1526 - val_loss: 0.1579 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1544 - val_loss: 0.1614 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1544 - val_loss: 0.1606 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1526 - val_loss: 0.1679 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1540 - val_loss: 0.1572 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1527 - val_loss: 0.1583 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1551 - val_loss: 0.1604 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1538 - val_loss: 0.1551 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1529 - val_loss: 0.1591 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1502 - val_loss: 0.1567 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1531 - val_loss: 0.1554 - learning_rate: 0.0010\n",
      "Epoch 123/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1503 - val_loss: 0.1595 - learning_rate: 0.0010\n",
      "Epoch 124/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1508 - val_loss: 0.1563 - learning_rate: 0.0010\n",
      "Epoch 125/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1528 - val_loss: 0.1593 - learning_rate: 0.0010\n",
      "Epoch 126/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1489 - val_loss: 0.1645 - learning_rate: 0.0010\n",
      "Epoch 127/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1493 - val_loss: 0.1529 - learning_rate: 0.0010\n",
      "Epoch 128/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1492 - val_loss: 0.1577 - learning_rate: 0.0010\n",
      "Epoch 129/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1487 - val_loss: 0.1603 - learning_rate: 0.0010\n",
      "Epoch 130/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1511 - val_loss: 0.1545 - learning_rate: 0.0010\n",
      "Epoch 131/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1493 - val_loss: 0.1529 - learning_rate: 0.0010\n",
      "Epoch 132/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1458 - val_loss: 0.1508 - learning_rate: 0.0010\n",
      "Epoch 133/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1474 - val_loss: 0.1630 - learning_rate: 0.0010\n",
      "Epoch 134/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1483 - val_loss: 0.1546 - learning_rate: 0.0010\n",
      "Epoch 135/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1426 - val_loss: 0.1549 - learning_rate: 0.0010\n",
      "Epoch 136/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1430 - val_loss: 0.1509 - learning_rate: 0.0010\n",
      "Epoch 137/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1430 - val_loss: 0.1513 - learning_rate: 0.0010\n",
      "Epoch 138/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1437 - val_loss: 0.1543 - learning_rate: 0.0010\n",
      "Epoch 139/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1437 - val_loss: 0.1506 - learning_rate: 0.0010\n",
      "Epoch 140/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1434 - val_loss: 0.1714 - learning_rate: 0.0010\n",
      "Epoch 141/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1464 - val_loss: 0.1491 - learning_rate: 0.0010\n",
      "Epoch 142/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1410 - val_loss: 0.1517 - learning_rate: 0.0010\n",
      "Epoch 143/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1385 - val_loss: 0.1465 - learning_rate: 0.0010\n",
      "Epoch 144/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1378 - val_loss: 0.1476 - learning_rate: 0.0010\n",
      "Epoch 145/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1364 - val_loss: 0.1495 - learning_rate: 0.0010\n",
      "Epoch 146/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1388 - val_loss: 0.1542 - learning_rate: 0.0010\n",
      "Epoch 147/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1369 - val_loss: 0.1466 - learning_rate: 0.0010\n",
      "Epoch 148/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1361 - val_loss: 0.1429 - learning_rate: 0.0010\n",
      "Epoch 149/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1375 - val_loss: 0.1498 - learning_rate: 0.0010\n",
      "Epoch 150/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1323 - val_loss: 0.1458 - learning_rate: 0.0010\n",
      "Epoch 151/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1343 - val_loss: 0.1490 - learning_rate: 0.0010\n",
      "Epoch 152/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1317 - val_loss: 0.1428 - learning_rate: 0.0010\n",
      "Epoch 153/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1353 - val_loss: 0.1463 - learning_rate: 0.0010\n",
      "Epoch 154/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1310 - val_loss: 0.1458 - learning_rate: 0.0010\n",
      "Epoch 155/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1291 - val_loss: 0.1376 - learning_rate: 0.0010\n",
      "Epoch 156/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1295 - val_loss: 0.1494 - learning_rate: 0.0010\n",
      "Epoch 157/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1274 - val_loss: 0.1432 - learning_rate: 0.0010\n",
      "Epoch 158/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1286 - val_loss: 0.1371 - learning_rate: 0.0010\n",
      "Epoch 159/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1227 - val_loss: 0.1433 - learning_rate: 0.0010\n",
      "Epoch 160/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1237 - val_loss: 0.1509 - learning_rate: 0.0010\n",
      "Epoch 161/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1236 - val_loss: 0.1382 - learning_rate: 0.0010\n",
      "Epoch 162/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1203 - val_loss: 0.1350 - learning_rate: 0.0010\n",
      "Epoch 163/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1185 - val_loss: 0.1346 - learning_rate: 0.0010\n",
      "Epoch 164/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1169 - val_loss: 0.1323 - learning_rate: 0.0010\n",
      "Epoch 165/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1145 - val_loss: 0.1387 - learning_rate: 0.0010\n",
      "Epoch 166/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1135 - val_loss: 0.1328 - learning_rate: 0.0010\n",
      "Epoch 167/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1095 - val_loss: 0.1230 - learning_rate: 0.0010\n",
      "Epoch 168/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1075 - val_loss: 0.1283 - learning_rate: 0.0010\n",
      "Epoch 169/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1073 - val_loss: 0.1262 - learning_rate: 0.0010\n",
      "Epoch 170/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1048 - val_loss: 0.1218 - learning_rate: 0.0010\n",
      "Epoch 171/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1024 - val_loss: 0.1274 - learning_rate: 0.0010\n",
      "Epoch 172/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1023 - val_loss: 0.1170 - learning_rate: 0.0010\n",
      "Epoch 173/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1005 - val_loss: 0.1229 - learning_rate: 0.0010\n",
      "Epoch 174/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0964 - val_loss: 0.1151 - learning_rate: 0.0010\n",
      "Epoch 175/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0918 - val_loss: 0.1127 - learning_rate: 0.0010\n",
      "Epoch 176/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0951 - val_loss: 0.1194 - learning_rate: 0.0010\n",
      "Epoch 177/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0898 - val_loss: 0.1101 - learning_rate: 0.0010\n",
      "Epoch 178/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0887 - val_loss: 0.1107 - learning_rate: 0.0010\n",
      "Epoch 179/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0854 - val_loss: 0.1056 - learning_rate: 0.0010\n",
      "Epoch 180/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0856 - val_loss: 0.1192 - learning_rate: 0.0010\n",
      "Epoch 181/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0846 - val_loss: 0.1033 - learning_rate: 0.0010\n",
      "Epoch 182/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0784 - val_loss: 0.1004 - learning_rate: 0.0010\n",
      "Epoch 183/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0790 - val_loss: 0.1085 - learning_rate: 0.0010\n",
      "Epoch 184/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0807 - val_loss: 0.1106 - learning_rate: 0.0010\n",
      "Epoch 185/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0759 - val_loss: 0.0953 - learning_rate: 0.0010\n",
      "Epoch 186/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0714 - val_loss: 0.1042 - learning_rate: 0.0010\n",
      "Epoch 187/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0695 - val_loss: 0.0901 - learning_rate: 0.0010\n",
      "Epoch 188/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0678 - val_loss: 0.0927 - learning_rate: 0.0010\n",
      "Epoch 189/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0657 - val_loss: 0.0893 - learning_rate: 0.0010\n",
      "Epoch 190/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0656 - val_loss: 0.0852 - learning_rate: 0.0010\n",
      "Epoch 191/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0616 - val_loss: 0.0841 - learning_rate: 0.0010\n",
      "Epoch 192/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0599 - val_loss: 0.0843 - learning_rate: 0.0010\n",
      "Epoch 193/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0592 - val_loss: 0.0806 - learning_rate: 0.0010\n",
      "Epoch 194/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0542 - val_loss: 0.0849 - learning_rate: 0.0010\n",
      "Epoch 195/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0541 - val_loss: 0.0862 - learning_rate: 0.0010\n",
      "Epoch 196/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0546 - val_loss: 0.0814 - learning_rate: 0.0010\n",
      "Epoch 197/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0496 - val_loss: 0.0767 - learning_rate: 0.0010\n",
      "Epoch 198/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0481 - val_loss: 0.0740 - learning_rate: 0.0010\n",
      "Epoch 199/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0490 - val_loss: 0.0743 - learning_rate: 0.0010\n",
      "Epoch 200/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0457 - val_loss: 0.0750 - learning_rate: 0.0010\n",
      "Epoch 201/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0470 - val_loss: 0.0724 - learning_rate: 0.0010\n",
      "Epoch 202/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0436 - val_loss: 0.0776 - learning_rate: 0.0010\n",
      "Epoch 203/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0426 - val_loss: 0.0692 - learning_rate: 0.0010\n",
      "Epoch 204/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0399 - val_loss: 0.0684 - learning_rate: 0.0010\n",
      "Epoch 205/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0375 - val_loss: 0.0704 - learning_rate: 0.0010\n",
      "Epoch 206/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0395 - val_loss: 0.0666 - learning_rate: 0.0010\n",
      "Epoch 207/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0415 - val_loss: 0.0660 - learning_rate: 0.0010\n",
      "Epoch 208/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0377 - val_loss: 0.0653 - learning_rate: 0.0010\n",
      "Epoch 209/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0385 - val_loss: 0.0641 - learning_rate: 0.0010\n",
      "Epoch 210/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0356 - val_loss: 0.0669 - learning_rate: 0.0010\n",
      "Epoch 211/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0364 - val_loss: 0.0711 - learning_rate: 0.0010\n",
      "Epoch 212/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0352 - val_loss: 0.0703 - learning_rate: 0.0010\n",
      "Epoch 213/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0344 - val_loss: 0.0610 - learning_rate: 0.0010\n",
      "Epoch 214/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0317 - val_loss: 0.0601 - learning_rate: 0.0010\n",
      "Epoch 215/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0290 - val_loss: 0.0592 - learning_rate: 0.0010\n",
      "Epoch 216/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0305 - val_loss: 0.0595 - learning_rate: 0.0010\n",
      "Epoch 217/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0302 - val_loss: 0.0641 - learning_rate: 0.0010\n",
      "Epoch 218/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0295 - val_loss: 0.0605 - learning_rate: 0.0010\n",
      "Epoch 219/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0267 - val_loss: 0.0588 - learning_rate: 0.0010\n",
      "Epoch 220/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0259 - val_loss: 0.0561 - learning_rate: 0.0010\n",
      "Epoch 221/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0256 - val_loss: 0.0592 - learning_rate: 0.0010\n",
      "Epoch 222/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0255 - val_loss: 0.0570 - learning_rate: 0.0010\n",
      "Epoch 223/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0253 - val_loss: 0.0608 - learning_rate: 0.0010\n",
      "Epoch 224/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0275 - val_loss: 0.0593 - learning_rate: 0.0010\n",
      "Epoch 225/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0240 - val_loss: 0.0546 - learning_rate: 0.0010\n",
      "Epoch 226/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0222 - val_loss: 0.0528 - learning_rate: 0.0010\n",
      "Epoch 227/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0220 - val_loss: 0.0525 - learning_rate: 0.0010\n",
      "Epoch 228/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0215 - val_loss: 0.0538 - learning_rate: 0.0010\n",
      "Epoch 229/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0223 - val_loss: 0.0537 - learning_rate: 0.0010\n",
      "Epoch 230/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0219 - val_loss: 0.0529 - learning_rate: 0.0010\n",
      "Epoch 231/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0214 - val_loss: 0.0488 - learning_rate: 0.0010\n",
      "Epoch 232/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0191 - val_loss: 0.0499 - learning_rate: 0.0010\n",
      "Epoch 233/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0203 - val_loss: 0.0498 - learning_rate: 0.0010\n",
      "Epoch 234/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0209 - val_loss: 0.0562 - learning_rate: 0.0010\n",
      "Epoch 235/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0211 - val_loss: 0.0560 - learning_rate: 0.0010\n",
      "Epoch 236/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0209 - val_loss: 0.0513 - learning_rate: 0.0010\n",
      "Epoch 237/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0191 - val_loss: 0.0505 - learning_rate: 0.0010\n",
      "Epoch 238/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0190 - val_loss: 0.0482 - learning_rate: 0.0010\n",
      "Epoch 239/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0172 - val_loss: 0.0506 - learning_rate: 0.0010\n",
      "Epoch 240/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0188 - val_loss: 0.0487 - learning_rate: 0.0010\n",
      "Epoch 241/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0171 - val_loss: 0.0506 - learning_rate: 0.0010\n",
      "Epoch 242/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0172 - val_loss: 0.0475 - learning_rate: 0.0010\n",
      "Epoch 243/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0182 - val_loss: 0.0487 - learning_rate: 0.0010\n",
      "Epoch 244/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0165 - val_loss: 0.0504 - learning_rate: 0.0010\n",
      "Epoch 245/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0166 - val_loss: 0.0452 - learning_rate: 0.0010\n",
      "Epoch 246/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0155 - val_loss: 0.0464 - learning_rate: 0.0010\n",
      "Epoch 247/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0148 - val_loss: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 248/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0163 - val_loss: 0.0510 - learning_rate: 0.0010\n",
      "Epoch 249/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0196 - val_loss: 0.0575 - learning_rate: 0.0010\n",
      "Epoch 250/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0194 - val_loss: 0.0488 - learning_rate: 0.0010\n",
      "Epoch 251/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0172 - val_loss: 0.0463 - learning_rate: 0.0010\n",
      "Epoch 252/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0151 - val_loss: 0.0459 - learning_rate: 0.0010\n",
      "Epoch 253/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0145 - val_loss: 0.0436 - learning_rate: 0.0010\n",
      "Epoch 254/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0135 - val_loss: 0.0440 - learning_rate: 0.0010\n",
      "Epoch 255/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0142 - val_loss: 0.0467 - learning_rate: 0.0010\n",
      "Epoch 256/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0168 - val_loss: 0.0463 - learning_rate: 0.0010\n",
      "Epoch 257/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0156 - val_loss: 0.0458 - learning_rate: 0.0010\n",
      "Epoch 258/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0154 - val_loss: 0.0455 - learning_rate: 0.0010\n",
      "Epoch 259/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0143 - val_loss: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 260/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0132 - val_loss: 0.0416 - learning_rate: 0.0010\n",
      "Epoch 261/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0136 - val_loss: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 262/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0137 - val_loss: 0.0468 - learning_rate: 0.0010\n",
      "Epoch 263/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0142 - val_loss: 0.0448 - learning_rate: 0.0010\n",
      "Epoch 264/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0149 - val_loss: 0.0473 - learning_rate: 0.0010\n",
      "Epoch 265/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0132 - val_loss: 0.0438 - learning_rate: 0.0010\n",
      "Epoch 266/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0146 - val_loss: 0.0451 - learning_rate: 0.0010\n",
      "Epoch 267/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0137 - val_loss: 0.0463 - learning_rate: 0.0010\n",
      "Epoch 268/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0123 - val_loss: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 269/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0121 - val_loss: 0.0412 - learning_rate: 0.0010\n",
      "Epoch 270/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0117 - val_loss: 0.0424 - learning_rate: 0.0010\n",
      "Epoch 271/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0130 - val_loss: 0.0439 - learning_rate: 0.0010\n",
      "Epoch 272/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0119 - val_loss: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 273/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0121 - val_loss: 0.0447 - learning_rate: 0.0010\n",
      "Epoch 274/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0123 - val_loss: 0.0437 - learning_rate: 0.0010\n",
      "Epoch 275/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0419 - learning_rate: 0.0010\n",
      "Epoch 276/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0103 - val_loss: 0.0396 - learning_rate: 0.0010\n",
      "Epoch 277/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0099 - val_loss: 0.0404 - learning_rate: 0.0010\n",
      "Epoch 278/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0108 - val_loss: 0.0500 - learning_rate: 0.0010\n",
      "Epoch 279/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0120 - val_loss: 0.0447 - learning_rate: 0.0010\n",
      "Epoch 280/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0141 - val_loss: 0.0451 - learning_rate: 0.0010\n",
      "Epoch 281/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0117 - val_loss: 0.0425 - learning_rate: 0.0010\n",
      "Epoch 282/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0102 - val_loss: 0.0444 - learning_rate: 0.0010\n",
      "Epoch 283/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0115 - val_loss: 0.0417 - learning_rate: 0.0010\n",
      "Epoch 284/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0101 - val_loss: 0.0405 - learning_rate: 0.0010\n",
      "Epoch 285/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0115 - val_loss: 0.0429 - learning_rate: 0.0010\n",
      "Epoch 286/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0128 - val_loss: 0.0411 - learning_rate: 0.0010\n",
      "Epoch 287/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0121 - val_loss: 0.0417 - learning_rate: 0.0010\n",
      "Epoch 288/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0115 - val_loss: 0.0434 - learning_rate: 0.0010\n",
      "Epoch 289/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0069 - val_loss: 0.0360 - learning_rate: 1.0000e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0049 - val_loss: 0.0357 - learning_rate: 1.0000e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0045 - val_loss: 0.0357 - learning_rate: 1.0000e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0044 - val_loss: 0.0358 - learning_rate: 1.0000e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0043 - val_loss: 0.0355 - learning_rate: 1.0000e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0043 - val_loss: 0.0354 - learning_rate: 1.0000e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0042 - val_loss: 0.0355 - learning_rate: 1.0000e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0042 - val_loss: 0.0355 - learning_rate: 1.0000e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0041 - val_loss: 0.0354 - learning_rate: 1.0000e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0041 - val_loss: 0.0353 - learning_rate: 1.0000e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0041 - val_loss: 0.0354 - learning_rate: 1.0000e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0041 - val_loss: 0.0353 - learning_rate: 1.0000e-04\n",
      "\n",
      "==================================================\n",
      "      ATLAS HIGH-PERFORMANCE AUDIT\n",
      "==================================================\n",
      "Batch Size:        1594 contracts\n",
      "Total Latency:     76.36 ms\n",
      "Calibration Error: 406.2262 bps\n",
      "Status:            ⚠️ REFINING\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- MASTER ATLAS PIPELINE (HIGH-FIDELITY) ---\n",
    "\n",
    "try:\n",
    "    # 1. Generate 150,000 Feller-Strict Samples\n",
    "    # Enforcing 2*kappa*theta > sigma^2 kills the \"noise\" at the variance boundary\n",
    "    generator = HHWGen(calibrator)\n",
    "    raw_data = generator.dataset(n_samples=150000, n_cores=4)\n",
    "\n",
    "    # 2. Data Refinery\n",
    "    print(\"Refining High-Fidelity Dataset...\")\n",
    "    data_prep = ATLASDataPrep(raw_data)\n",
    "    xt, xv, yt, yv = data_prep.prepare_tensors()\n",
    "\n",
    "    # 3. Train Wide-Body Neural Engine\n",
    "    # Constant 512-width to capture complex 10D correlations\n",
    "    engine = ATLASNeuralEngine(input_shape=(10,))\n",
    "    print(\"Engaging Wide-Body Training (A100 Backend)...\")\n",
    "    history = engine.train(xt, yt, xv, yv, Nepochs=300)\n",
    "\n",
    "    # 4. Final Production Audit\n",
    "    run_atlas_audit(engine, data_prep, calibrator, clean_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ATLAS Pipeline Failure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7d2a3-07f1-4697-931c-75a0898fda77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
